= Graph Redistribution Manager

Graph Redistribution Manager (GRM) is a proof of concept solution of data redistribution in distributed graph database systems.
The process of redistribution starts with loading a dataset in JanusGraph database. Then, a set of generated queries is
ran against the database, generating a processed result log via the custom Processed Result Logging module of the TinkerPop project.
The paths from the log are added to the database. Last but not least, a partitioning algorithm is executed and a new partition for each
vertex is proposed.

== VaqueroVertexProgram

``VaqueroVertexProgram`` is the Vaquero et al.footnote:[Vaquero, L. M.; Cuadrado, F.; et al. Adaptive Partitioning for Large-Scale
 Dynamic Graphs. In 2014 IEEE 34th International Conference on Distributed Computing Systems, Madrid, Spain, 2014, pp. 144â€“153,
  doi:10.1109/ICDCS.2014.23.]
algorithm implemented in the TinkerPop ```VertexProgram``` framework which is bases on Pregel computation system.
Therefore, the ```VaqueroVertexProgram``` works in iterations. After initial setup, the ```execute``` method is ran for each vertex of the graph.
These iterations are ran
in parallel manner, which also means that after each iteration, there is a synchronization barrier and the ```terminate``` method is called.
The ```terminate``` method determines whether the algorithm should continue into the next iteration or whether the program should halt.
After the end of the execution,
each vertex is assigned a new partition ID which can be accessed via the ```vertex.value(PARTITION)``` method. The ```PARTITION```
is a constant with the key of the property of the new partition ID.

=== Iteration process
In each iteration, vertices communicate with each other via messages. Every vertex sends its partition ID (PID) to all of its neighbors.
A new partition ID of the vertex is determined by the frequency of PIDs of the vertex neighbors.
The more frequently the PID is represented among the neighbors, the higher the chance that it will be acquired.
However, there are more conditions which determine whether the new PID will be acquired.
One of them is the current balance of the partitions. The ```imbalanceFactor``` sets the lower bound of how much the partitions
can be imbalanced regarding a proportional vertex distribution.
The upper bound of each partition is determined by its maximum capacity.
Another condition solves an oscillation problem of two or more vertices changing their PID back and forth.

== Datasets

The current implementation supports two datasets for testing the Vaquero et al. algorithm.

  * Twitter dataset - https://snap.stanford.edu/data/roadNet-PA.html[link]
  * Pennsylvania road network dataset - https://snap.stanford.edu/data/ego-Twitter.html[link]

Loading and query testing is implemented via two classes: ```PennsylvaniaDatasetLoaderQueryRunner``` and ```TwitterDatasetLoaderQueryRunner```.

=== Configuration and usage
The ```VaqueroVertexProgram``` can be built with the ```VaqueroVertexProgram.Builder``` which enables configuration of the algorithm.
[source,java]
----
 vertexProgram = VaqueroVertexProgram.build()
                     .clusterMapper(cm)
                     .acquirePartitionProbability(0.5)
                     .imbalanceFactor(0.80)
                     .coolingFactor(0.99)
                     .adoptionFactor(1)
                     .evaluatingMap(runner.evaluatingMap())
                     .evaluateCrossCommunication(true)
                     .scopeIncidentTraversal(__.outE())
                     .evaluatingStatsOriginal(runner.evaluatingStats())
                     .maxPartitionChangeRatio(1)
                     .maxIterations(200)
                 .create(graph);
 algorithmResult = graph.compute().program(vertexProgram).workers(12).submit().get();
----
In the code snippet above, an instance of the ```VaqueroVertexProgram``` is created with the ```VaqueroVertexProgram.Builder```,
 which is then submitted to the ```GraphComputer```.

Configuration::
  * ```clusterMapper``` - sets the provided ```ClusterMapper``` to determine the original partition
    ID from a vertex ID.
  * ```acquirePartitionProbability``` - sets the probability for vertices to acquire a new partition.
  * ```imbalanceFactor``` - sets the imbalance factor which determines how much the partitions can be imbalanced regarding the lower bound.
  * ```coolingFactor``` - simulated annealing is used when executing the program. The cooling factor determines how quickly the temperature
   is going to drop, affecting the probability of vertices, which partition to acquire.
  * ```adoptionFactor``` - determines how to calculate the probability of acquiring a new partition.
  * ```evaluatingMap``` - provides a map of vertices to number of their appearance in the query set.
  It is used for evaluating the improvement of the partitioning between iterations.
  * ```evaluateCrossCommunication``` - sets whether or not to enable cross-node communication evaluation between iterations.
  * ```scopeIncidentTraversal``` - sets the incident vertices to which to send a message ( ```__.outE()``` means that the
  messages will be sent to incident vertices using only out-going edges).
  * ```evaluatingStatsOriginal``` - sets the initial statistics of cross-node communication.
  * ```maxPartitionChangeRatio``` - sets the limit of how many vertices can change partition during one iteration
  (input is a fraction of the vertex count).
  * ```maxIterations``` - sets the maximum number of iterations before the program halts.

Another important configuration is selecting the proper graph configuration file. In ```config.properties``` you can
currently state either ```graph-berkeley-embeded.properties``` or ```graph-cassandra-embedded.properties``` for
the ```graph.propFile``` property. This provides the BerkeleyDB or the embedded Apache Cassandra storage backend, respectively.

Project structure::
    logHandling:::
      * ```PRLogFileLoader``` - loads a log file in the memory or provides an ```Iterator<PRLogRecord>``` of the log file.
      * ```PRLog``` - represents a log file when loaded in the memory.
      * ```PRLogRecord``` - represents all paths generated by one ```Traverser```, that is, by one query.
      * ```PRPath``` - list of ```PRElement``` s representing one path of the ```Traverser```.
      * ```PRElement``` - represents an edge or a vertex of the graph.
      * ```PRLogToGraphLoader``` - this interface describes operations necessary for loading a log in the graph,  which are ```addSchema```, ```removeSchema``` and ```loadLogToGraph```.
      * ```DefaultPRLogToGraphLoader``` - the default implementation of the ```PRLogToGraphLoader``` loading the ```PRLog``` to a given graph.
    dataset:::
      * ```DatasetQueryRunner``` - defines an interface to run a set of queries against the database.
      * ```DatasetLoader``` - defines an interface to load a dataset in the graph database.

      * ```PennsylvaniaDatasetLoaderQueryRunner``` - implements both the interfaces above. Handles the Pennsylvania road network dataset.

      * ```TwitterDatasetLoaderQueryRunner``` - implements both the interfaces above. Handles the Twitter network dataset.
    cluster:::
      * ```PartitionMapper``` - this interface defines the method ```map(vertexID)``` to get the partition ID from ```vertexID```.
      * ```DefaultPartitionMapper``` - a default implementation of the ```PartitionMapper```, which uses modulo division of the hash
      of ```vertexID``` (that was before converted to ```originalID``` via ```IDManager.fromVertexId```) to get the partition ID.

    helpers:::
      * ```HelperOperator``` - a set of binary operators to use when reducing data in the ```VaqueroVertexProgram```.
      * ```ShuffleComparator``` - a random comparator which is used for traversal ordering when implementing a random walk on the graph.
    GRMP::: Executable class that runs the complete "benchmark" of the Pennsylvania road network dataset.
    GRMT::: Executable class that runs the complete "benchmark" of the Twitter network dataset.
    GRM::: The base class of the ```GRMP``` and the ```GRMT```, containing shared components and resources.
